% StereoComplex paper draft (synthetic benchmark only).
\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{url}

\title{Compression-Robust Stereo Calibration via Planar Geometric Refinement of Fiducial Corners}
\author{Author(s)\\
\small Affiliation(s)\\
\small Corresponding author: email@example.com}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Fiducial-based stereo calibration is often developed and evaluated on pristine imagery, yet practical pipelines frequently operate on lossy-compressed streams (bandwidth-limited transmission, storage constraints, onboard logging).
Lossy compression introduces spatially structured artifacts that yield biased subpixel localization of fiducial corners; these errors are not well modeled as independent noise and can destabilize epipolar geometry and depth estimates.
We propose a model-agnostic two-stage planar refinement that regularizes 2D correspondences before calibration: a robust homography fit on the calibration plane followed by a smooth residual warp learned from marker correspondences.
We evaluate on a fully reproducible synthetic benchmark with ground-truth geometry and controlled degradations, including codec-quality sweeps, and report stereo-relevant metrics (vertical disparity/rectification stability, baseline error in disparity-equivalent pixels, and triangulation stability when ground truth is available).
On our compression sweep, an optional compact 3D ray-based stereo model further improves robustness, yielding triangulation error that remains stable across lossy compression levels compared to pinhole-based baselines.
Code, benchmark scripts, and configurations are released for reproducibility.
\end{abstract}

\textbf{Keywords:} stereo calibration; ChArUco; compression robustness; homography; thin-plate spline; ray-based stereo; reproducible benchmark

\section{Introduction}
\subsection{Problem statement}
Stereo calibration from planar fiducial targets (e.g., ChArUco boards) is a widely used workflow in robotics and metrology.
However, most calibration pipelines are developed under an implicit assumption of high-quality imagery (RAW or lossless formats).
In practical systems, images may be recorded or transmitted with lossy compression: on-board video streams, compressed logging, or industrial camera pipelines that only provide compressed output.

Lossy compression does not simply add independent noise.
Block artifacts and ringing around sharp edges induce spatially structured biases near fiducial corners, which can persist after subpixel refinement.
These structured biases can yield unstable epipolar geometry, increased vertical disparity after rectification, and degraded triangulation.
In many such regimes, \emph{2D localization quality} becomes the bottleneck for stereo geometry, independent of the downstream camera model.

\subsection{Key idea}
We treat corner localization as a geometric estimation problem on the calibration plane.
Rather than directly feeding raw detections into a camera model fit, we introduce a \emph{planar second pass}: we estimate a robust planar mapping and a smooth residual warp to denoise the correspondence set before calibration and reconstruction.
This refinement is model-agnostic (it does not require known intrinsics) and is designed to absorb structured, low-frequency biases introduced by degradations such as compression.

\subsection{Contributions}
We provide the following falsifiable contributions:
\begin{itemize}
  \item \textbf{C1 (Planar refinement).} A two-stage planar geometric refinement pipeline (robust homography + smooth residual warp) that improves correspondence consistency under degradations (compression, blur, noise) before stereo calibration.
  \item \textbf{C2 (Stereo metrics).} A stereo evaluation protocol with metrics that directly quantify stereo usefulness: vertical disparity after rectification, baseline error in disparity-equivalent pixels, and triangulation stability (and ray skew) when ground truth is available.
  \item \textbf{C3 (Synthetic benchmark).} A synthetic benchmark suite with controlled degradations (including compression sweeps) and ground-truth geometry enabling fully reproducible comparisons.
  \item \textbf{C4 (Optional ray-based stereo).} A compact \emph{central} ray-based stereo model (Zernike basis + regularized point$\leftrightarrow$ray optimization) that yields improved robustness under compression compared to pinhole-only calibration on our benchmark.
\end{itemize}

\subsection{Paper organization}
Section~\ref{sec:related} reviews related work.
Section~\ref{sec:formulation} formulates how structured 2D biases propagate to stereo geometry errors.
Section~\ref{sec:method2d} presents the planar second-pass refinement.
Section~\ref{sec:ray3d} describes an optional compact central ray-based stereo model.
Sections~\ref{sec:benchmark}--\ref{sec:experiments} describe the synthetic benchmark and experiments (synthetic only).
Section~\ref{sec:discussion} discusses limitations and future work.

\section{Related work}
\label{sec:related}
\subsection{Fiducial-based camera and stereo calibration}
Planar calibration targets and Zhang-style methods~\cite{zhang2000flexible} form the backbone of many camera calibration workflows.
ChArUco boards combine chessboards with ArUco markers for robust detection under partial occlusion~\cite{garrido2014aruco,romero2018charuco}.
OpenCV provides widely used implementations~\cite{opencv}, typically optimizing parameters by minimizing reprojection error.

\subsection{Robust estimation and correspondence regularization}
Robust estimation techniques such as RANSAC~\cite{fischler1981ransac} and M-estimators (e.g., Huber loss~\cite{huber1964robust}) are standard tools for outlier rejection and robust fitting.
For planar correspondence sets, geometric regularization can be performed by fitting parametric plane mappings and smoothing residual fields.

\subsection{Calibration under degradations}
Blur and noise effects on calibration have been studied extensively; however, compression artifacts are not well modeled by independent noise.
Lossy compression can induce coherent local biases (e.g., ringing) that systematically affect corner localization.
This work focuses on the resulting stereo consequences and on geometric regularization strategies that remain effective under such structured perturbations.

\subsection{Ray-based and model-agnostic camera representations}
General imaging models and ray-based calibration have been explored as alternatives to simple pinhole models, including compact representations and point-based calibration under generalized models~\cite{miraldo2011pointbased}.
Here we restrict to a \emph{central} ray-based model as a practical extension that avoids explicit pinhole intrinsics while remaining compact and optimizable.

\section{Problem formulation: from 2D localization error to stereo geometry error}
\label{sec:formulation}
\subsection{Notation}
We consider a planar calibration target with known board-plane coordinates $\mathbf{x}=(x,y)^\top$.
For a given camera view, the observed image measurement is a 2D point $\mathbf{u}=(u,v)^\top$ in pixels.
We denote homogeneous coordinates by $\tilde{\mathbf{x}}=(x,y,1)^\top$ and $\tilde{\mathbf{u}}=(u,v,1)^\top$ and use projective division $\pi$:
\begin{equation}
\pi\!\left((a,b,c)^\top\right)=\left(\frac{a}{c},\frac{b}{c}\right)^\top,\qquad c\neq 0.
\end{equation}

Let $\mathbf{u}^\star(\mathbf{x})$ denote the ideal projection (including geometric distortion) of board point $\mathbf{x}$ into the image.
We model an observed corner as
\begin{equation}
\mathbf{u}(\mathbf{x}) = \mathbf{u}^\star(\mathbf{x}) + \mathbf{b}(\mathbf{x}) + \boldsymbol{\varepsilon},
\label{eq:obs_model}
\end{equation}
where $\mathbf{b}(\mathbf{x})$ is a structured bias (e.g., induced by compression artifacts) and $\boldsymbol{\varepsilon}$ is residual noise and occasional outliers.
Crucially, $\mathbf{b}(\mathbf{x})$ can be spatially coherent and low-frequency over the board plane.

\subsection{Why stereo suffers}
Stereo reconstruction relies on consistent correspondences and stable epipolar geometry.
Small biases in $\mathbf{u}_L,\mathbf{u}_R$ translate into vertical disparity after rectification (violating the epipolar assumption), and into disparity errors that affect depth.
If calibration is performed on biased 2D measurements, a pinhole model may compensate by distorting intrinsics/distortion/extrinsics, potentially reducing reprojection RMS while degrading stereo geometry (e.g., baseline drift).
This motivates metrics that reflect stereo usefulness beyond reprojection RMS.

\subsection{Stereo usefulness metrics}
We report the following stereo-relevant metrics:
\begin{itemize}
  \item \textbf{Stereo reprojection RMS} (OpenCV fit objective; included for completeness).
  \item \textbf{Baseline error in disparity-equivalent pixels:} translate baseline error into an equivalent disparity error at a representative depth $\bar Z$ to interpret scale stability.
  \item \textbf{Triangulation RMS (when ground truth is available):} report RMS 3D error, and a depth-normalized version ($\%\,\bar Z$) for comparability across scenes.
  \item \textbf{Ray skew} (for ray-based stereo): the minimum distance between the two back-projected rays (should be small when geometry is consistent).
\end{itemize}
Formal definitions and derivations are provided in Appendix~\ref{app:metrics}.

\section{Method: planar geometric refinement (``second pass'')}
\label{sec:method2d}
\subsection{Pipeline overview}
Given an image, we detect ArUco markers and recover a set of board-plane to image correspondences
$\{(\mathbf{x}_i,\mathbf{u}_i)\}_{i=1}^N$ from marker corners.
We then:
\begin{enumerate}
  \item Estimate a robust homography $H$ mapping the board plane to the image.
  \item Compute residuals $\mathbf{r}_i=\mathbf{u}_i-\pi(H\tilde{\mathbf{x}}_i)$.
  \item Fit a smooth residual warp $\mathbf{g}(\mathbf{x})$ on the board plane and predict $\hat{\mathbf{u}}(\mathbf{x})=\pi(H\tilde{\mathbf{x}})+\mathbf{g}(\mathbf{x})$ for all chessboard corners.
  \item Feed refined correspondences to downstream stereo calibration and evaluation.
\end{enumerate}

\subsection{Robust homography estimation}
We estimate $H\in\mathbb{R}^{3\times 3}$ using RANSAC~\cite{fischler1981ransac} on the marker-corner correspondences:
\begin{equation}
\tilde{\mathbf{u}} \sim H\tilde{\mathbf{x}}.
\end{equation}
RANSAC rejects gross outliers (e.g., occasional marker misdetections), yielding an initial plane mapping robust to outliers.

\subsection{Smooth residual warp model}
We model the residual correction $\mathbf{g}(\mathbf{x})\in\mathbb{R}^2$ as a smooth function on the board plane.
In this work we use a thin-plate spline (TPS) residual field~\cite{bookstein1989principal} with a robust loss:
\begin{equation}
\min_{\mathbf{g}} \sum_{i=1}^N \rho\!\left(\left\|\mathbf{r}_i-\mathbf{g}(\mathbf{x}_i)\right\|^2\right) + \lambda\,\mathcal{R}_{\mathrm{TPS}}(\mathbf{g}),
\label{eq:tps_objective}
\end{equation}
where $\rho$ is a robust penalty (Huber~\cite{huber1964robust}) and $\mathcal{R}_{\mathrm{TPS}}$ is the bending-energy regularizer controlling smoothness.
We solve Eq.~\eqref{eq:tps_objective} by iteratively reweighted least squares (IRLS), updating weights from current residual magnitudes.
The smoothness parameter $\lambda$ trades off fidelity to the observed residuals and spatial smoothness, and is swept in ablations (Appendix~\ref{app:irls_tps}).

\subsection{Implementation notes and scope}
The method is planar: it assumes correspondences lie on the calibration plane and that residual biases vary smoothly over that plane.
It is therefore most appropriate for refining planar target measurements (calibration, rectification, and plane-based diagnostics).
Extrapolation far outside the convex hull of detected markers is not guaranteed and is treated as a failure mode.

\subsection{2D synthetic evaluation of corner identification}
We first evaluate planar refinement as a pure 2D identification method by comparing predicted chessboard corners to ground-truth projections on a synthetic dataset (no real data).
Table~\ref{tab:charuco_methods} reports representative 2D errors for multiple identification strategies.
\input{tables/charuco_methods.tex}

\section{Optional extension: compact central ray-based stereo}
\label{sec:ray3d}
\subsection{Model definition}
We define a \emph{central} ray-based camera model that maps a pixel $(u,v)$ to a unit direction $\mathbf{d}(u,v)\in\mathbb{R}^3$.
We parameterize
\begin{equation}
\mathbf{d}(u,v) = \frac{1}{\left\| (x(u,v),y(u,v),1)^\top \right\|}\,(x(u,v),y(u,v),1)^\top,
\end{equation}
where $x(u,v)$ and $y(u,v)$ are expanded in a real Zernike basis on a unit disk.
We regularize coefficients with an $\ell_2$ penalty to enforce smoothness and limit overfitting.
This central model is a stepping stone towards more general (non-central) ray models; we do not claim non-central support in this paper.

\subsection{Fitting procedure (point--ray optimization)}
Given multiple planar poses, we associate each observed pixel with a known 3D board point $\mathbf{P}$ (in the board frame) and estimate per-frame poses $(R_i,\mathbf{t}_i)$.
For a central camera, the point in camera coordinates is $\mathbf{P}_{i}=R_i\mathbf{P}+\mathbf{t}_i$.
We fit ray-field parameters by minimizing point-to-ray residuals:
\begin{equation}
\mathbf{r}_{i} = \left(I - \mathbf{d}\mathbf{d}^\top\right)\mathbf{P}_{i},
\end{equation}
with robust loss and coefficient regularization.
For stereo, we jointly optimize a single rigid transform between cameras and per-frame board poses, minimizing the sum of left/right point-to-ray residuals.

\subsection{Stereo triangulation and skew}
Given a correspondence $(u_L,v_L)\leftrightarrow(u_R,v_R)$, we back-project two rays and triangulate by the closest approach of the two lines.
We report the resulting 3D estimate and the \emph{ray skew} (minimum distance between rays) as a geometry-consistency diagnostic.

\section{Synthetic benchmark design}
\label{sec:benchmark}
\subsection{Ground-truth geometry}
We generate synthetic stereo scenes with known calibration target geometry, camera poses, and ground-truth projections.
The benchmark provides:
(i) ground-truth 2D corner projections per view, and
(ii) ground-truth 3D target points for triangulation evaluation.

\subsection{Degradation operators}
We apply controlled degradations to the rendered images:
\begin{itemize}
  \item \textbf{Compression:} re-encode images with multiple codecs and quality levels (lossless PNG; lossy WebP/JPEG quality sweeps).
  \item \textbf{Blur/noise:} optional Gaussian blur and additive noise (used in additional ablations; not the focus of this paper).
\end{itemize}
Compression is treated as a first-class experimental variable because it yields structured artifacts at high-contrast edges.

\subsection{Protocol}
We use a fixed number of poses per scene and evaluate identical frame subsets across codec variants.
All scripts are deterministic given a random seed and are released with the benchmark.

\section{Experiments (synthetic only)}
\label{sec:experiments}
\subsection{Baselines}
We compare:
\begin{itemize}
  \item \textbf{B1: OpenCV pinhole (raw).} Detect corners and run standard pinhole stereo calibration and triangulation.
  \item \textbf{B2: OpenCV pinhole + planar refinement.} Apply the planar second pass, then run the same OpenCV calibration.
  \item \textbf{B3: 3D ray-field + planar refinement.} Use planar refinement for 2D correspondences, then fit the compact central ray-based stereo model and triangulate by ray intersection.
\end{itemize}

\subsection{Main results: robustness to compression}
Figure~\ref{fig:compression_tri} reports triangulation stability (RMS depth error normalized by mean depth) as a function of codec quality.
The compact 3D ray-field is remarkably stable across compression levels on this benchmark, while pinhole-based baselines remain sensitive to codec artifacts.

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{../docs/assets/compression_sweep/tri_rms_rel_depth_percent.png}
\caption{Compression sweep (synthetic): triangulation RMS as a percentage of mean depth versus codec quality. The 3D ray-field remains stable across codec settings on this benchmark, while pinhole-based pipelines are sensitive to lossy compression.}
\label{fig:compression_tri}
\end{figure}

We also report stereo RMS and baseline error in disparity-equivalent pixels (Figures~\ref{fig:compression_stereo_rms}--\ref{fig:compression_baseline}).

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{../docs/assets/compression_sweep/stereo_rms_px.png}
\caption{Compression sweep (synthetic): stereo reprojection RMS versus codec quality for the OpenCV pipelines. Reprojection RMS alone does not fully characterize stereo usefulness under compression.}
\label{fig:compression_stereo_rms}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{../docs/assets/compression_sweep/baseline_abs_error_px_at_mean_depth.png}
\caption{Compression sweep (synthetic): baseline absolute error converted to disparity-equivalent pixels at mean depth. Pinhole baselines exhibit codec-dependent drift, whereas the 3D ray-field remains comparatively stable.}
\label{fig:compression_baseline}
\end{figure}

\subsection{Summary table (selected codecs)}
Table~\ref{tab:compression_summary} summarizes representative metrics for a lossless PNG and selected lossy WebP/JPEG settings using the released sweep results.

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{llccc}
\toprule
Codec & Method & Stereo RMS (px) & Baseline err (px @ $\bar Z$) & Tri RMS (\%$\bar Z$)\\
\midrule
\multicolumn{5}{l}{\textbf{PNG (lossless)}}\\
\quad & OpenCV pinhole (raw) & 0.529 & 1.693 & 0.971\\
\quad & OpenCV pinhole + planar refinement & 0.123 & 0.287 & 1.079\\
\quad & 3D ray-field + planar refinement & -- & 0.200 & 0.097\\
\midrule
\multicolumn{5}{l}{\textbf{WebP (q70)}}\\
\quad & OpenCV pinhole (raw) & 0.499 & 1.208 & 0.874\\
\quad & OpenCV pinhole + planar refinement & 0.117 & 0.110 & 1.010\\
\quad & 3D ray-field + planar refinement & -- & 0.218 & 0.077\\
\midrule
\multicolumn{5}{l}{\textbf{JPEG (q80)}}\\
\quad & OpenCV pinhole (raw) & 0.513 & 1.376 & 0.890\\
\quad & OpenCV pinhole + planar refinement & 0.119 & 0.137 & 1.102\\
\quad & 3D ray-field + planar refinement & -- & 0.296 & 0.101\\
\bottomrule
\end{tabular}
\caption{Compression robustness (synthetic, selected codecs) using the released sweep results (\texttt{docs/assets/compression\_sweep/sweep\_metrics.json}). ``Baseline err'' is converted to disparity-equivalent pixels at mean depth $\bar Z$; ``Tri RMS'' is depth-normalized. The 3D ray-field remains stable across lossy codecs in this benchmark.}
\label{tab:compression_summary}
\end{table}

\subsection{Why reprojection RMS can be misleading}
Under structured perturbations, a pinhole model may partially compensate biased correspondences by drifting intrinsics/extrinsics, thereby reducing reprojection RMS without improving rectification stability or triangulation.
This motivates reporting baseline-in-pixels and triangulation stability in addition to reprojection RMS.

\subsection{Runtime considerations}
Planar refinement is lightweight once correspondences are detected: it requires a homography fit and a small TPS solve on the marker-corner set.
The central ray-field optimization is more expensive (nonlinear least squares over ray-field coefficients, per-frame poses, and rig parameters) but remains practical for the small number of calibration frames used in our benchmark.
In production, the ray-field model evaluates with costs comparable to a standard camera model (compute a small basis and a few dot products per pixel).

\section{Discussion and limitations}
\label{sec:discussion}
\subsection{Why planar refinement helps under compression}
Compression artifacts induce structured, spatially correlated biases near edges.
On a planar target, these biases are well captured by a low-frequency residual field on the plane, which can be learned robustly from marker-corner correspondences.
The planar refinement therefore improves correspondence consistency before calibration, reducing the propagation of structured 2D errors into stereo geometry.

\subsection{Parameter truth versus epipolar stability}
Improving stereo usefulness (rectification and depth stability) does not necessarily imply more accurate intrinsic parameters.
Under structured measurement biases, different parameterizations can trade off intrinsic ``truth'' for extrinsic/epipolar consistency.
We therefore emphasize stereo-relevant metrics and provide ground-truth comparisons in synthetic experiments.

\subsection{Scope and future work}
This paper reports \emph{synthetic-only} results with ground-truth geometry and controlled degradations.
Real-world validation (including sensor pipelines and non-ideal optics) is left as future work.
The current ray-based model is central; extending to non-central generalized cameras is also future work.

\section{Reproducibility and open-source release}
All experiments in this paper are reproducible from the released repository.
Key scripts include:
\begin{itemize}
  \item Dataset generation and re-encoding utilities (synthetic benchmark and codec variants).
  \item Compression sweep driver and plotting scripts producing Figures~\ref{fig:compression_tri}--\ref{fig:compression_baseline}.
  \item Corner identification comparisons producing Table~\ref{tab:charuco_methods}.
\end{itemize}
The code is released under GPL-2.0-or-later; documentation is released under CC BY-SA 4.0 (see repository licensing files).

\section{Conclusion}
We presented a planar geometric refinement pipeline that improves the stability of fiducial correspondences under structured degradations such as lossy compression.
Using a fully reproducible synthetic benchmark, we demonstrated that stereo-relevant metrics (baseline stability in pixels and triangulation stability) reveal failure modes not captured by reprojection RMS alone.
An optional compact central ray-based stereo model further improves robustness to compression on our benchmark.
Real-world validation and extensions to non-central ray models are left as future work.

\appendix
\section{Metric definitions}
\label{app:metrics}
This appendix summarizes the core stereo metrics used in the experiments.

\paragraph{Baseline error in disparity-equivalent pixels.}
Given an estimated baseline error $\Delta B$ (in mm) and a representative depth $\bar Z$ (in mm), we report a disparity-equivalent pixel error by mapping baseline error to its effect on disparity at $\bar Z$ under the nominal geometry.
This provides an interpretable pixel-domain scale stability metric.

\paragraph{Triangulation RMS.}
When ground-truth 3D points are available, we compute RMS 3D error in mm, and a depth-normalized version:
\begin{equation}
\mathrm{TriRMS}_{\%} = 100\,\frac{\sqrt{\frac{1}{N}\sum_i \| \hat{\mathbf{P}}_i-\mathbf{P}_i\|^2}}{\bar Z}.
\end{equation}

\paragraph{Ray skew.}
For ray-based stereo, we also report the minimum distance between the two rays corresponding to a match; this quantifies how close the rays come to intersecting.

\section{IRLS-TPS refinement details}
\label{app:irls_tps}
Equation~\eqref{eq:tps_objective} is solved by IRLS with a robust Huber penalty~\cite{huber1964robust}.
At each iteration, weights are updated from current residual magnitudes and a weighted TPS linear system is solved.
Implementation follows a standard smoothing spline formulation, with $\lambda$ controlling smoothness.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
